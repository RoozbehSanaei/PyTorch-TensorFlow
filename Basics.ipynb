{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2tLZFsr8+0hWH+7NZV0UI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoozbehSanaei/PyTorch-TensorFlow/blob/gh-pages/Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBA_03nTXkdD"
      },
      "source": [
        "## Simple Linear Regression Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJMn4XFor2fZ"
      },
      "source": [
        "*Generating Data*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kglEDEEwlgVE"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct_s6rgTNaVM"
      },
      "source": [
        "*Numpy*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzSxKyKdaFdl"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Create random input and output data\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "# Randomly initialize weights\n",
        "a = np.random.uniform()\n",
        "b = np.random.uniform()\n",
        "c = np.random.uniform()\n",
        "d = np.random.uniform()\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.mean(np.square(y_pred - y))\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_a = grad_y_pred.sum()\n",
        "    grad_b = (grad_y_pred * x).sum()\n",
        "    grad_c = (grad_y_pred * x ** 2).sum()\n",
        "    grad_d = (grad_y_pred * x ** 3).sum()\n",
        "    \n",
        "    # Update weights\n",
        "    a -= learning_rate * grad_a\n",
        "    b -= learning_rate * grad_b\n",
        "    c -= learning_rate * grad_c\n",
        "    d -= learning_rate * grad_d\n",
        "\n",
        "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbhYz33QXrj6"
      },
      "source": [
        "*TensorFlow*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRwHaoWjXrj8"
      },
      "source": [
        "import tensorflow as tf\n",
        "class LinearRegressionKeras(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.w3 = tf.Variable(tf.random.uniform(shape=[1]))\n",
        "        self.w2 = tf.Variable(tf.random.uniform(shape=[1]))\n",
        "        self.w1 = tf.Variable(tf.random.uniform(shape=[1]))\n",
        "        self.b = tf.Variable(tf.random.uniform(shape=[1]))\n",
        "\n",
        "    def __call__(self,x): \n",
        "        return  x * x * x * self.w3 + x * x * self.w2 + x * self.w1 + self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXOqRJrdjghC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awe7V36JFB26"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "tf_model = LinearRegressionKeras()\n",
        "[w3, w2, w1, b] = tf_model.trainable_variables\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    \n",
        "    y_pred = tf_model(x)    \n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_b = tf.reduce_sum(grad_y_pred)\n",
        "    grad_w1 = tf.reduce_sum(grad_y_pred * x)\n",
        "    grad_w2 = tf.reduce_sum(grad_y_pred * x ** 2)\n",
        "    grad_w3 =  tf.reduce_sum(grad_y_pred * x ** 3)\n",
        "\n",
        "\n",
        "    # Update weights\n",
        "    b.assign(b-learning_rate * grad_b)\n",
        "    w1.assign(w1-learning_rate * grad_w1)\n",
        "    w2.assign(w2-learning_rate * grad_w2)\n",
        "    w3.assign(w3-learning_rate * grad_w3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiL2CLQT4Cjx"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "tf_model = LinearRegressionKeras()\n",
        "[w3, w2, w1, b] = tf_model.trainable_variables\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "learning_rate = 0.002\n",
        "\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = tf_model(x)    \n",
        "        loss = squared_error(y_pred, y)\n",
        "    # Compute and print loss\n",
        "    print(loss)\n",
        "\n",
        "    grad_w3,grad_w2,grad_w1,grad_b  = tape.gradient(loss, tf_model.trainable_variables)\n",
        "\n",
        "\n",
        "    # Update weights\n",
        "    b.assign(b-learning_rate * grad_b)\n",
        "    w1.assign(w1-learning_rate * grad_w1)\n",
        "    w2.assign(w2-learning_rate * grad_w2)\n",
        "    w3.assign(w3-learning_rate * grad_w3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JamJWXYCJNAw"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "learning_rate = 0.002\n",
        "\n",
        "tf_model = LinearRegressionKeras()\n",
        "[w3, w2, w1, b] = tf_model.trainable_variables\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = tf_model(x)    \n",
        "        loss = squared_error(y_pred, y)\n",
        "    # Compute and print loss\n",
        "    print(loss)\n",
        "\n",
        "    grads = tape.gradient(loss, tf_model.trainable_variables)\n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, tf_model.variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adgY-dPk7NlP"
      },
      "source": [
        "class LinearRegressionKeras(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = tf.keras.layers.Dense(1, activation=None) # , input_shape=[1]\n",
        "\n",
        "    def call(self, x): \n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "learning_rate = 0.002\n",
        "\n",
        "tf_model = LinearRegressionKeras()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "tf_model_train_loop = LinearRegressionKeras()\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "for epoch in range(2000):\n",
        "    x_batch = tf.reshape(x, [2000, 1])\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = tf_model_train_loop(x_batch)\n",
        "        y_pred = tf.reshape(y_pred, [2000])\n",
        "        loss = tf.losses.mse(y_pred, y)\n",
        "    \n",
        "    grads = tape.gradient(loss, tf_model_train_loop.variables)\n",
        "    \n",
        "    optimizer.apply_gradients(grads_and_vars=zip(grads, tf_model_train_loop.variables))\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch} : Loss {loss.numpy()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQdetMfJXrj9"
      },
      "source": [
        "*PyTorch*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEi8kQcpzRGX"
      },
      "source": [
        "import torch\n",
        "class LinearRegressionPyTorch(torch.nn.Module): \n",
        "    def __init__(self): \n",
        "        super().__init__() \n",
        "        self.w3 = torch.nn.Parameter(torch.Tensor(1, 1).uniform_())\n",
        "        self.w2 = torch.nn.Parameter(torch.Tensor(1, 1).uniform_())\n",
        "        self.w1 = torch.nn.Parameter(torch.Tensor(1, 1).uniform_())\n",
        "        self.b = torch.nn.Parameter(torch.Tensor(1).uniform_())\n",
        "    def forward(self, x):  \n",
        "        return  x**3@self.w3 + x**2@self.w2 + x @ self.w1 + self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Si3j-4pSaeN"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "x = torch.from_numpy(x.reshape(-1, 1))\n",
        "y = torch.from_numpy(y.reshape(-1, 1))\n",
        "\n",
        "torch_model = LinearRegressionPyTorch()\n",
        "[w3, w2, w1, b] =  torch_model.parameters()\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "  return tf.reduce_mean(tf.square(y_pred - y_true))\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(2000):\n",
        "    # Forward pass: compute predicted y\n",
        "    # y = a + b x + c x^2 + d x^3\n",
        "    y_pred = torch_model(x)    \n",
        "\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_b = torch.sum(grad_y_pred)\n",
        "    grad_w1 = torch.sum(grad_y_pred * x)\n",
        "    grad_w2 = torch.sum(grad_y_pred * x ** 2)\n",
        "    grad_w3 =  torch.sum(grad_y_pred * x ** 3)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "    # Update weights\n",
        "        b -= learning_rate * grad_b\n",
        "        w1 -= learning_rate * grad_w1\n",
        "        w2 -= learning_rate * grad_w2\n",
        "        w3 -= learning_rate * grad_w3\n",
        "\n",
        "    print(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GmiSG4AjzOW"
      },
      "source": [
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "x = torch.from_numpy(x.reshape(-1, 1))\n",
        "y = torch.from_numpy(y.reshape(-1, 1))\n",
        "\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "    return torch.mean(torch.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "torch_model = LinearRegressionPyTorch()\n",
        "[w3, w2, w1, b] =  torch_model.parameters()\n",
        "\n",
        "learning_rate = 0.002\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(2000):\n",
        "    y_pred = torch_model(x)\n",
        "    loss = squared_error(y_pred, y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w1 -= w1.grad * learning_rate\n",
        "        w2 -= w2.grad * learning_rate\n",
        "        w3 -= w3.grad * learning_rate\n",
        "        b -= b.grad * learning_rate\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        w3.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "    print(f\"Epoch {epoch} : Loss {loss.data}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB4pxLvQxbqt"
      },
      "source": [
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "x = torch.from_numpy(x.reshape(-1, 1))\n",
        "y = torch.from_numpy(y.reshape(-1, 1))\n",
        "\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "    return torch.mean(torch.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "torch_model = LinearRegressionPyTorch()\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimizer = torch.optim.SGD(torch_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(2000):\n",
        "    y_pred = torch_model(x)\n",
        "    loss = squared_error(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "      print(f\"Epoch {epoch} : Loss {loss.data}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwUEZD2s3EVR"
      },
      "source": [
        "class LinearRegressionPyTorch(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearRegressionPyTorch, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "def generate_data(n=2000):\n",
        "    x = np.random.uniform(-math.pi,math.pi, n)\n",
        "    noise = np.random.normal(0, 0.15, n)\n",
        "    y = np.sin(x) + noise\n",
        "    return x.astype(np.float32), y.astype(np.float32)\n",
        "\n",
        "x, y = generate_data()\n",
        "\n",
        "x = torch.from_numpy(x.reshape(-1, 1))\n",
        "y = torch.from_numpy(y.reshape(-1, 1))\n",
        "\n",
        "\n",
        "def squared_error(y_pred, y_true):\n",
        "    return torch.mean(torch.square(y_pred - y_true))\n",
        "\n",
        "\n",
        "torch_model = LinearRegressionPyTorch()\n",
        "\n",
        "criterion = torch.nn.MSELoss(reduction='mean')\n",
        "optimizer = torch.optim.SGD(torch_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(20000):\n",
        "    y_pred = torch_model(x)\n",
        "    loss = squared_error(y_pred, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "      print(f\"Epoch {epoch} : Loss {loss.data}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}